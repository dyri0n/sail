# =============================================================================
# CONFIGURACIÓN DE ENTORNO - AIRFLOW ETL NODE
# =============================================================================
# Copia este archivo a .env y ajusta los valores según tu entorno.
# NUNCA commitees el archivo .env con credenciales reales.
#
# USO:
#   1. Copia: cp .env.example .env
#   2. Ajusta valores según tu entorno (testing local o producción remota)
#   3. Las variables se inyectan automáticamente en docker-compose.yaml
#   4. Airflow-init las usa para crear conexiones automáticamente
# =============================================================================

# =============================================================================
# SECCIÓN 1: CREDENCIALES ADMIN DE AIRFLOW
# =============================================================================
# Usuario y contraseña para acceder a la UI de Airflow (http://localhost:8080)
# El servicio 'airflow-init' crea este usuario automáticamente al iniciar.
# -----------------------------------------------------------------------------
AIRFLOW_ADMIN_USERNAME=admin
AIRFLOW_ADMIN_PASSWORD=admin
AIRFLOW_ADMIN_FIRSTNAME=Admin
AIRFLOW_ADMIN_LASTNAME=User
AIRFLOW_ADMIN_EMAIL=admin@ejemplo.cl

# UID del usuario airflow dentro del contenedor (permisos de archivos)
AIRFLOW_UID=50000

# =============================================================================
# SECCIÓN 2: DATA WAREHOUSE (DWH) - BASE DE DATOS PRODUCCIÓN
# =============================================================================
# El DWH almacena las tablas dimensionales y de hechos (modelo Kimball).
# Los DAGs de Airflow ejecutan transformaciones SQL en esta base.
#
# CUÁNDO USAR CADA HOST:
#   - host.docker.internal: Desde contenedor Airflow hacia host Windows/Mac
#   - localhost: Desde tu máquina (fuera de Docker) hacia el contenedor DWH
#   - dwh_rrhh_container: Si DWH y Airflow están en la misma red Docker
#
# TESTING LOCAL (dwh-node levantado en mismo host):
#   DWH_HOST=host.docker.internal
#   DWH_PORT=6000
#
# PRODUCCIÓN (DWH en servidor remoto):
#   DWH_HOST=192.168.1.50
#   DWH_PORT=6000
# -----------------------------------------------------------------------------
DWH_HOST=host.docker.internal
DWH_PORT=6000
DWH_USER=dwh_admin
DWH_PASSWORD=sail-rrhh-p4
DWH_DATABASE=rrhh_prod

# --- MODO MOCK LOCAL: DWH simulado para testear compilación ---
# DWH_HOST=mock-dwh
# DWH_PORT=5432
# DWH_USER=test_user
# DWH_PASSWORD=test_password
# DWH_DATABASE=test_dwh

# =============================================================================
# SECCIÓN 3: STAGING DATABASE - BASE DE DATOS INTERMEDIA (2 MODOS)
# =============================================================================
# Staging es donde se cargan los datos crudos antes de transformarlos.
# Tenemos 2 contenedores de staging:
#
#   1. STAGING PROD (sin datos):
#      - Puerto 6001
#      - Base de datos: rrhh_staging
#      - Uso: Para flujos reales de ETL con datos de entrada
#
#   2. STAGING TEST (con datos de prueba precargados):
#      - Puerto 6002
#      - Base de datos: rrhh_staging_test
#      - Uso: Para desarrollo y testing de DAGs sin datos reales
#
# CONFIGURACIÓN PARA TESTING LOCAL (usar staging con datos de prueba):
# -----------------------------------------------------------------------------

# --- MODO TESTING: Staging con datos de prueba (puerto 6002) ---
STAGING_HOST=host.docker.internal
STAGING_PORT=6002
STAGING_USER=stg_admin
STAGING_PASSWORD=sail-stg-p4
STAGING_DATABASE=rrhh_staging_test

# --- MODO PRODUCCIÓN: Staging vacío (descomenta y cambia puerto a 6001) ---
# STAGING_HOST=host.docker.internal
# STAGING_PORT=6001
# STAGING_USER=stg_admin
# STAGING_PASSWORD=sail-stg-p4
# STAGING_DATABASE=rrhh_staging

# --- MODO MOCK LOCAL: Staging simulado para testear compilación ---
# STAGING_HOST=mock-staging
# STAGING_PORT=5432
# STAGING_USER=test_user
# STAGING_PASSWORD=test_password
# STAGING_DATABASE=test_staging

# =============================================================================
# SECCIÓN 4: AIRFLOW CONNECTIONS (IDs de conexión internos)
# =============================================================================
# Nombres de las conexiones que Airflow crea automáticamente en la UI.
# Puedes referenciarlos en tus DAGs con: PostgresHook(postgres_conn_id='...')
# No cambies estos IDs a menos que sepas lo que haces.
# -----------------------------------------------------------------------------
AIRFLOW_DWH_CONN_ID=dwh_postgres_conn
AIRFLOW_STAGING_CONN_ID=staging_postgres_conn

# =============================================================================
# SECCIÓN 5: ETL WORKERS - CONTENEDORES REMOTOS PARA PROCESAMIENTO PESADO
# =============================================================================
# Cuando un DAG necesita ejecutar Python pesado (lecturas de Excel, pandas),
# puede lanzar contenedores efímeros usando DockerOperator.
# Estos contenedores se ejecutan fuera de Airflow y se destruyen al terminar.
#
# TESTING LOCAL:
#   DOCKER_WORKER_URL=unix://var/run/docker.sock  (mismo host)
#
# PRODUCCIÓN (worker remoto):
#   DOCKER_WORKER_URL=tcp://192.168.1.60:2375
# -----------------------------------------------------------------------------
ETL_WORKER_IMAGE=mi-sistema/etl-worker:latest
DOCKER_WORKER_URL=unix://var/run/docker.sock
DOCKER_NETWORK_MODE=bridge

# Límites de recursos para cada contenedor worker
WORKER_MEM_LIMIT=2g
WORKER_CPUS=1.0

# =============================================================================
# SECCIÓN 6: RUTAS DE DATOS (Volúmenes montados en workers)
# =============================================================================
# Los workers necesitan acceder a archivos de entrada (Excel, CSV).
# Estas rutas mapean directorios del host a directorios dentro del contenedor.
#
# TESTING LOCAL (Windows):
#   DATA_MOUNT_SOURCE=D:/Code/SAIL/etl-node/input_data
#
# PRODUCCIÓN (Linux):
#   DATA_MOUNT_SOURCE=/var/etl/input_data
# -----------------------------------------------------------------------------
DATA_MOUNT_SOURCE=/tmp/etl_data
DATA_MOUNT_TARGET=/app/data
