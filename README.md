# ğŸš€ SAIL - Sistema de Analytics e Inteligencia Laboral

<p align="center">
  <img src="https://img.shields.io/badge/PostgreSQL-17-336791?style=for-the-badge&logo=postgresql" alt="PostgreSQL">
  <img src="https://img.shields.io/badge/Apache%20Airflow-3.1.5-017CEE?style=for-the-badge&logo=apache-airflow" alt="Airflow">
  <img src="https://img.shields.io/badge/Docker-24+-2496ED?style=for-the-badge&logo=docker" alt="Docker">
  <img src="https://img.shields.io/badge/Python-3.11-3776AB?style=for-the-badge&logo=python" alt="Python">
</p>

---

## ğŸ“‹ DescripciÃ³n

**SAIL** es un Data Warehouse moderno para el Ã¡rea de **Recursos Humanos**, diseÃ±ado con arquitectura **Kimball** (modelo estrella). Permite centralizar, transformar y analizar datos de:

- ğŸ‘¥ **DotaciÃ³n y RotaciÃ³n** de personal
- ğŸ“š **Capacitaciones** y desarrollo
- ğŸ“Š **MÃ©tricas de RRHH** (turnover, headcount, etc.)

---

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              SAIL - ARQUITECTURA                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   ğŸ“ INPUT_DATA  â”‚  Archivos Excel/CSV
     â”‚  (Landing Zone)  â”‚  depositados manualmente
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ETL-NODE (OrquestaciÃ³n)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    ğŸŒ¬ï¸ APACHE AIRFLOW 3.x                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  Scheduler  â”‚  â”‚  Webserver  â”‚  â”‚DAG Processorâ”‚  â”‚  Postgres â”‚   â”‚   â”‚
â”‚  â”‚  â”‚             â”‚  â”‚   :8080     â”‚  â”‚  (parser)   â”‚  â”‚ (metadata)â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚                                        â”‚
â”‚  â”‚ ğŸ”¨ ETL-WORKERS   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  DockerOperator                       â”‚
â”‚  â”‚  (Python/Pandas) â”‚     (orquesta contenedores efÃ­meros)                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                      SQL + ConexiÃ³n        â”‚
                      (dwh_postgres_conn)   â”‚
                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DWH-NODE (PostgreSQL 17)                           â”‚
â”‚                                                                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚                    ğŸ“¦ rrhh_prod (Database)                    â”‚       â”‚
â”‚     â”‚                                                               â”‚       â”‚
â”‚     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚       â”‚
â”‚     â”‚   â”‚    Schema: stg      â”‚    â”‚    Schema: dwh      â”‚         â”‚       â”‚
â”‚     â”‚   â”‚   (Staging Area)    â”‚    â”‚   (Data Warehouse)  â”‚         â”‚       â”‚
â”‚     â”‚   â”‚                     â”‚    â”‚                     â”‚         â”‚       â”‚
â”‚     â”‚   â”‚ â€¢ stg_rotacion_*    â”‚â”€â”€â”€â–¶â”‚ â€¢ dim_tiempo        â”‚         â”‚       â”‚
â”‚     â”‚   â”‚ â€¢ stg_capacitacionesâ”‚    â”‚ â€¢ dim_empleado      â”‚         â”‚       â”‚
â”‚     â”‚   â”‚ â€¢ stg_feriados      â”‚    â”‚ â€¢ dim_cargo         â”‚         â”‚       â”‚
â”‚     â”‚   â”‚                     â”‚    â”‚ â€¢ dim_empresa       â”‚         â”‚       â”‚
â”‚     â”‚   â”‚   (Datos crudos,    â”‚    â”‚ â€¢ dim_gerencia      â”‚         â”‚       â”‚
â”‚     â”‚   â”‚    temporales)      â”‚    â”‚ â€¢ dim_centro_costo  â”‚         â”‚       â”‚
â”‚     â”‚   â”‚                     â”‚    â”‚ â€¢ fact_rotacion     â”‚         â”‚       â”‚
â”‚     â”‚   â”‚                     â”‚    â”‚ â€¢ fact_dotacion     â”‚         â”‚       â”‚
â”‚     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                Puerto: 6000                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Estructura del Proyecto

```
SAIL/
â”œâ”€â”€ ğŸ“‚ dwh-node/                    # Nodo de Base de Datos
â”‚   â”œâ”€â”€ docker-compose.yaml         # OrquestaciÃ³n PostgreSQL
â”‚   â”œâ”€â”€ Dockerfile                  # Imagen producciÃ³n
â”‚   â”œâ”€â”€ Dockerfile.testing          # Imagen con datos de prueba
â”‚   â””â”€â”€ init-scripts/               # Scripts SQL de inicializaciÃ³n
â”‚       â”œâ”€â”€ 01-users.sql            # Usuarios y roles
â”‚       â”œâ”€â”€ 02-schema-stg.sql       # Schema staging
â”‚       â”œâ”€â”€ 03-schema-dwh.sql       # Schema DWH (dims + facts)
â”‚       â””â”€â”€ 04-seed-test-data.sql   # Datos de prueba (testing)
â”‚
â”œâ”€â”€ ğŸ“‚ etl-node/                    # Nodo de Procesamiento ETL
â”‚   â”œâ”€â”€ ğŸ“‚ airflow/                 # Apache Airflow (orquestador)
â”‚   â”‚   â”œâ”€â”€ docker-compose.yaml     # Stack completo Airflow 3.x
â”‚   â”‚   â”œâ”€â”€ Dockerfile              # Imagen con providers
â”‚   â”‚   â”œâ”€â”€ .env.example            # Variables de entorno ejemplo
â”‚   â”‚   â”œâ”€â”€ dags/                   # Flujos de trabajo
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_conformed.py    # DAG: Dimensiones conformadas
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_rotacion.py     # DAG: Facts de rotaciÃ³n
â”‚   â”‚   â”‚   â”œâ”€â”€ config/settings.py  # ConfiguraciÃ³n centralizada
â”‚   â”‚   â”‚   â””â”€â”€ sql/                # Scripts SQL de transformaciÃ³n
â”‚   â”‚   â”‚       â”œâ”€â”€ dimensiones/    # MERGE de dims
â”‚   â”‚   â”‚       â””â”€â”€ fact-tables/    # Carga de facts
â”‚   â”‚   â””â”€â”€ logs/                   # Logs de ejecuciÃ³n
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ etl-workers/             # Workers Python (cÃ³mputo pesado)
â”‚   â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ scripts/                # Scripts de extracciÃ³n
â”‚   â”‚       â””â”€â”€ extract_feriados.py # API feriados Chile
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ input_data/              # Landing zone (archivos fuente)
â”‚
â””â”€â”€ ğŸ“‚ scripts/                     # Scripts de gestiÃ³n global
    â”œâ”€â”€ start-all.ps1 / .sh         # Levantar toda la infra
    â””â”€â”€ stop-all.ps1 / .sh          # Detener toda la infra
```

---

## ğŸš€ Quick Start

### Prerrequisitos

- **Docker Desktop** 24+ con Docker Compose v2
- **PowerShell** 7+ (Windows) o **Bash** (Linux/Mac)
- **Git** para clonar el repositorio

### 1ï¸âƒ£ Clonar y Configurar

```powershell
# Clonar repositorio
git clone https://github.com/dyri0n/sail.git
cd sail

# Configurar variables de entorno para Airflow
cd etl-node/airflow
cp .env.example .env
# Editar .env si es necesario (ver secciÃ³n ConfiguraciÃ³n)
cd ../..
```

### 2ï¸âƒ£ Levantar Infraestructura Completa

```powershell
# Windows (PowerShell)
.\scripts\start-all.ps1

# Linux/Mac
./scripts/start-all.sh
```

### 3ï¸âƒ£ Verificar Servicios

| Servicio           | URL                   | Credenciales                 |
| ------------------ | --------------------- | ---------------------------- |
| **Airflow UI**     | http://localhost:8080 | `admin` / `admin`            |
| **PostgreSQL DWH** | `localhost:6000`      | `dwh_admin` / `sail-rrhh-p4` |

### 4ï¸âƒ£ Ejecutar tu Primer DAG

1. Accede a http://localhost:8080
2. Activa el DAG `01_carga_dimensiones_conformadas`
3. Haz clic en â–¶ï¸ **Trigger DAG**
4. Observa la ejecuciÃ³n en el Graph View

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (Airflow)

Copia `etl-node/airflow/.env.example` a `.env` y ajusta:

```dotenv
# ============ CREDENCIALES AIRFLOW ============
AIRFLOW_ADMIN_USERNAME=admin
AIRFLOW_ADMIN_PASSWORD=tu_password_seguro
AIRFLOW_FERNET_KEY=genera_con_script

# ============ CONEXIÃ“N AL DWH ============
# Testing local (DWH en mismo host)
DWH_HOST=host.docker.internal
DWH_PORT=6000

# ProducciÃ³n (DWH en servidor remoto)
# DWH_HOST=192.168.1.50
# DWH_PORT=6000

DWH_USER=dwh_admin
DWH_PASSWORD=sail-rrhh-p4
DWH_DATABASE=rrhh_prod
```

### Generar Fernet Key (Seguridad)

```powershell
# Windows
.\etl-node\airflow\scripts\generate-fernet-key.ps1

# Linux/Mac
./etl-node/airflow/scripts/generate-fernet-key.sh
```

---

## ğŸ”„ Pipeline ETL

### DAGs Disponibles

| DAG                                    | DescripciÃ³n                               | Schedule |
| -------------------------------------- | ----------------------------------------- | -------- |
| `01_carga_dimensiones_conformadas`     | Carga dims: tiempo, empleado, cargo, etc. | `@daily` |
| `02_carga_hechos_movimientos_dotacion` | Carga facts: rotaciÃ³n, dotaciÃ³n snapshot  | `@daily` |

### Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Fuentes   â”‚â”€â”€â”€â–¶â”‚   Staging   â”‚â”€â”€â”€â–¶â”‚     DWH     â”‚
â”‚  (Excel/API)â”‚    â”‚  (stg.*)    â”‚    â”‚ (dim/fact)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Paso 1            Paso 2             Paso 3
   (Workers)      (SQL Transform)     (Consumo BI)
```

---

## ğŸ—„ï¸ Modelo de Datos

### Dimensiones (Schema: `dwh`)

| Tabla                    | DescripciÃ³n                   | Tipo       |
| ------------------------ | ----------------------------- | ---------- |
| `dim_tiempo`             | Calendario con feriados Chile | EstÃ¡tica   |
| `dim_empleado`           | Maestro de empleados          | SCD Tipo 2 |
| `dim_cargo`              | Cargos y familias de puesto   | EstÃ¡tica   |
| `dim_empresa`            | Sociedades/empresas           | EstÃ¡tica   |
| `dim_gerencia`           | Estructura organizacional     | EstÃ¡tica   |
| `dim_centro_costo`       | Centros de costo              | EstÃ¡tica   |
| `dim_modalidad_contrato` | Junk: Tipo empleo + Jornada   | Junk       |

### Tablas de Hechos (Schema: `dwh`)

| Tabla           | DescripciÃ³n               | Granularidad       |
| --------------- | ------------------------- | ------------------ |
| `fact_rotacion` | Movimientos (altas/bajas) | Transaccional      |
| `fact_dotacion` | Foto mensual headcount    | Snapshot periÃ³dico |

---

## ğŸ› ï¸ Comandos Ãštiles

### Docker

```powershell
# Ver contenedores activos
docker ps

# Logs de Airflow Scheduler
docker logs airflow-scheduler -f

# Logs del DWH
docker logs dwh_rrhh_container -f

# Conectar a PostgreSQL DWH
docker exec -it dwh_rrhh_container psql -U dwh_admin -d rrhh_prod
```

### Airflow CLI (dentro del contenedor)

```bash
# Listar DAGs
docker exec airflow-scheduler airflow dags list

# Trigger manual
docker exec airflow-scheduler airflow dags trigger 01_carga_dimensiones_conformadas

# Ver estado de tareas
docker exec airflow-scheduler airflow tasks list 01_carga_dimensiones_conformadas
```

### Detener Todo

```powershell
# Windows
.\scripts\stop-all.ps1

# Linux/Mac
./scripts/stop-all.sh
```

---

## ğŸ§ª Testing

### Modo Testing (con datos de prueba)

```powershell
# Levantar DWH con seed data
cd dwh-node
$env:DOCKERFILE = "Dockerfile.testing"
docker compose up -d --build
```

### Worker Interactivo

```powershell
cd etl-node/etl-workers
docker compose --profile development up -d
docker exec -it etl-worker-shell bash
```

---

## ğŸ“Š ConexiÃ³n desde Herramientas BI

### Power BI / Tableau / Metabase

```
Host: localhost (o IP del servidor)
Puerto: 6000
Base de datos: rrhh_prod
Usuario: dwh_admin
Password: sail-rrhh-p4
Schema: dwh
```

### Python (Pandas/SQLAlchemy)

```python
from sqlalchemy import create_engine
import pandas as pd

engine = create_engine(
    "postgresql+psycopg2://dwh_admin:sail-rrhh-p4@localhost:6000/rrhh_prod"
)

# Ejemplo: Leer dimensiÃ³n empleados
df = pd.read_sql("SELECT * FROM dwh.dim_empleado WHERE scd_es_actual = true", engine)
```

---

## ğŸ¤ ContribuciÃ³n

1. Fork el repositorio
2. Crea una rama feature: `git checkout -b feature/nueva-dimension`
3. Commit cambios: `git commit -m 'Add dim_ubicacion'`
4. Push a la rama: `git push origin feature/nueva-dimension`
5. Abre un Pull Request

---

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ“ Soporte

- ğŸ“§ Email: [soporte@ejemplo.cl](mailto:soporte@ejemplo.cl)
- ğŸ› Issues: [GitHub Issues](https://github.com/dyri0n/sail/issues)

---

<p align="center">
  <sub>Built with â¤ï¸ by Data Team</sub>
</p>
